{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc729fd8-0671-465f-9853-aa9bc13f40cc",
   "metadata": {},
   "source": [
    "# Lecture 15a. Intro to Large Language Models\n",
    "\n",
    "Learning Goals:  \n",
    "\n",
    "* What is an LLM, and how important is it.\n",
    "    * Understand LLM classification, use-cases, tasks and different LLM versions.\n",
    "    * Basic Model Parameters (parameters size, input tokens, output tokens)\n",
    "\n",
    "\n",
    "- Understand fundamental terminology, concepts about LLMs.\n",
    "    - Prompt,\n",
    "    - Inference,\n",
    "    - Tokenization,\n",
    "    - Encoder, Decoder\n",
    "    - Embeddings       \n",
    "\n",
    "\n",
    "\n",
    "* Use an LLM.\n",
    "    * Install, model, weights\n",
    "    * Run an installed model\n",
    "    * Prompt,\n",
    "    * Inference\n",
    "    * Zero-shot, One-shot, Few-shot\n",
    "\n",
    "\n",
    "      \n",
    "- LLM training cycle:  \n",
    "    * Train, Inference, evaluate, finetune.\n",
    "\n",
    "\n",
    "* Understand Hardware, time and financial cost considerations.\n",
    "    - RAM\n",
    "    - Compute\n",
    "    - Dataset size\n",
    "\n",
    "         \n",
    "- Finetune:\n",
    "    * Prompt engineering,\n",
    "    * PEFT, LoRA\n",
    "\n",
    "\n",
    "* Try different models and datasets using Huggingface and Lamini.\n",
    "\n",
    "\n",
    "\n",
    "* Lectures material and next steps to knowledge.\n",
    "    * Human Reenforcement Learning,\n",
    "    * RAG\n",
    "    * Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632d709-3c4b-4316-9dbc-b7ae9c04c347",
   "metadata": {},
   "source": [
    "### LLMs and AI\n",
    "* What is an LLM, and how important is this depelopment towards AI systems.  \n",
    "    * Understand LLM classification, use-cases and different LLM versions.  \n",
    "\n",
    "Large Languare Models = Very Large in terms of Parameters models that do Natural Language Processing Tasks.     \n",
    "\n",
    "**Generative AI**: Creative but lacks self-learning ability.   \n",
    "\n",
    "**Artificial General Intelligence AGI or General AI**:  Self-learn and adapt to new situations in a way similar or better better than humans\n",
    "Is this AI, is it intelligent or just Machine Learning and if is is intelligent, is it sentient?  \n",
    "Can it wash the dishes so that I could do creative stuff, or will it do the creative stuff and humans will do only boring, hard manual labour?    \n",
    "Who knows the ancient quote about \"windmills and slavery\"?     \n",
    "\n",
    "**Super intelligence (Super AI)**: far beyond General AI and human intelligence. Theoretically, could be million times faster and smarter than humans, beyond our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ea07a-7c34-47fc-bfdc-98de387075b2",
   "metadata": {},
   "source": [
    "### LLMs and NPL tasks. Chosing the right model for the task.\n",
    "[Hugging Face NLP tasks](https://huggingface.co/tasks)\n",
    "\n",
    "Task Variants are subsets of the general category \"Task\".  \n",
    "Task Token Classification: Named Entity Recognition, Part of Speech.\n",
    "Task Text Classification: Sentiment Analysis, Grammatical Correctness.\n",
    "\n",
    "**LLMs are good in most NLP** tasks as measured by industry standards, benchmark tests:  \n",
    "Rankings and comparisons of LLMs based on various benchmark tests:\n",
    "\n",
    "1. **Papers with Code**:\n",
    "    - **Website**: [Papers with Code](https://paperswithcode.com/sota)\n",
    "    - **Description**: This site provides a comprehensive list of state-of-the-art results on various NLP benchmarks, including detailed comparisons and leaderboards for many of the benchmark tests mentioned above.\n",
    "    - **Features**: Benchmark leaderboards, links to papers, and code repositories.\n",
    "\n",
    "2. **SuperGLUE Leaderboard**:\n",
    "    - **Website**: [SuperGLUE Leaderboard](https://super.gluebenchmark.com/leaderboard)\n",
    "    - **Description**: Displays the performance of various models on the SuperGLUE benchmark, which includes more challenging language understanding tasks than GLUE.\n",
    "    - **Features**: Detailed performance metrics and comparisons.\n",
    "\n",
    "3. **GLUE Benchmark**:\n",
    "    - **Website**: [GLUE Benchmark](https://gluebenchmark.com/leaderboard)\n",
    "    - **Description**: Provides a leaderboard for the GLUE benchmark, showing how different models perform on general language understanding tasks.\n",
    "    - **Features**: Performance scores for individual tasks and aggregated scores.\n",
    "\n",
    "4. **Stanford Question Answering Dataset (SQuAD) Leaderboard**:\n",
    "    - **Website**: [SQuAD Leaderboard](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "    - **Description**: Lists the top-performing models on the SQuAD 1.1 and SQuAD 2.0 benchmarks.\n",
    "    - **Features**: Exact match and F1 scores for various models.\n",
    "\n",
    "5. **EleutherAI Language Model Evaluation Harness**:\n",
    "    - **Website**: [EleutherAI LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "    - **Description**: A framework for evaluating language models on a variety of benchmarks, with results published for different models.\n",
    "    - **Features**: Open-source evaluation framework, detailed results for multiple benchmarks.\n",
    "      \n",
    "6. Hugging Face [Open LLMs Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12ad33-738e-4cd9-aff8-2216b1c6ed24",
   "metadata": {},
   "source": [
    "### Pretrained models that have an advantege at specific tasks.  \n",
    "What to look for next:   \n",
    "Dataset availabilty, data quality, max input tokens, max output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9317e-3c74-43a7-ae5c-2ae7eb7fc75f",
   "metadata": {},
   "source": [
    "### fundamental terminology, concepts on LLMs usage.\n",
    "* Understand fundamental terminology, concepts about LLMs.\n",
    "    * Prompt,\n",
    "    * Inference,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74670ec-feb8-4738-926a-eac1c18364a8",
   "metadata": {},
   "source": [
    "    * Tokenization,\n",
    "\n",
    "    Practical Implications\n",
    "\n",
    "    Token Count: The number of tokens a piece of text is converted into will vary based on the tokenization method. This affects the input and output token limits you need to consider.\n",
    "    Model Selection: When selecting a model, ensure you understand how its tokenizer works to accurately estimate the token count for your specific inputs.\n",
    "\n",
    "Summary\n",
    "\n",
    "    Words vs. Tokens: A token does not necessarily correspond to a word; it can be a subword, character, or byte-pair encoded segment.\n",
    "    Spaces as Tokens: Spaces often count as tokens or are included in tokens, depending on the tokenizer.\n",
    "    Token Count Estimation: Use the tokenizer specific to the model to count tokens accurately for your inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffbf89-2ef6-4c0e-a446-55094110061c",
   "metadata": {},
   "source": [
    "    * Encoder, Decoder\n",
    "    * Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facd3e5-440e-4374-915b-6eb9b16f52de",
   "metadata": {},
   "source": [
    "* Using an LLM.\n",
    "    * Install, model, weights\n",
    "    * Run an installed model\n",
    "    * Prompt,\n",
    "    * Inference\n",
    "    * Zero-shot, One-shot, Few-shot\n",
    "    * Hyper parameters: tokens, temperature.\n",
    "    * Hallucination and jailbreaking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a3970-1920-4333-a11e-0229af169da0",
   "metadata": {},
   "source": [
    "* LLM training cycle:  \n",
    "    * Train, Inference, evaluate, finetune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d0d1-214d-4710-b0b5-f0143bd32934",
   "metadata": {},
   "source": [
    "* Understand Hardware, time and financial cost considerations.\n",
    "    * RAM\n",
    "    * Compute\n",
    "    * Dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a9862-fb06-4671-ba7e-696c14a90d42",
   "metadata": {},
   "source": [
    "* Finetune:\n",
    "    * Prompt engineering,\n",
    "    * PEFT, LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582cce0d-b48e-4c1b-81b3-41366611c975",
   "metadata": {},
   "source": [
    "* Try different models and datasets using Huggingface and Lamini.\n",
    "    * Sign up, get API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d416a-ac34-437e-b126-a5fb3bfcebcc",
   "metadata": {},
   "source": [
    "* Lectures material and next steps to knowledge.\n",
    "    * Human Reenforcement Learning,\n",
    "    * RAG\n",
    "    * Agents\n",
    "    * ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07714bbf-1ac6-4448-ab7f-1f361a05dd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
