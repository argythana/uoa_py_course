{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fef7d5b-0330-49e8-962c-31846317612b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Features and dataset topics\n",
    "\n",
    "## Domain knowledge matters. Know your data. Is imagination more important?     \n",
    "The same feature behaves differently in different phenomena.  \n",
    "The same feature may not behave the same across categories within the same phenomenon.  \n",
    "E.g. age and wage, age and strength, age and taste.   \n",
    "\n",
    "\n",
    "## Data size, shape, dimension, obs/features ratio, sparse data.\n",
    "[Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)   \n",
    "Sparse data: too few observations in groups of data => no common values in features.   \n",
    "Simple non-scientific rule: at least 10 rows to 1 feature (most of the times). But no less than 100 Obs (for one feature).   \n",
    "There might also be sampling representation bias (Data on weekends not the same as work days).   \n",
    "And there is also data quality, signal-to-noise ratio.  \n",
    "To conclude: as features increase => exponential growth of necessary observations, not linear growth.     \n",
    "\n",
    "\n",
    "## Add, create, engineer new features. From existing features, or from new data.\n",
    "New features might be linear combinations of existing features.   \n",
    "Created features => maybe better representation of the issue in question.   \n",
    "But, linear combinations means that using all of these features might create instead of solving problems.     \n",
    "Understand what linear combination is.    \n",
    "\n",
    "Examples: Square meters in houses, Body Mass Index (ratio of height to weight).   \n",
    "Create new categories, using EDA, Descritive stats and clustering.    \n",
    "\n",
    "## When large variance in the data.   \n",
    "Maybe (MAYBE) there are undocumented categories. Find them and group the data.     \n",
    "Maybe there are important features that are missing that explain the variance.\n",
    "E.g: Expensive areas to buy a house. Geo data may be used the create clusters.  \n",
    "E.g: Areas with imperfect markets (clusters of different pricing). This is a generalization of the above.   \n",
    "\n",
    "\n",
    "## [Impute missing values](https://scikit-learn.org/stable/modules/impute.html#impute) numerical, or categorical.\n",
    "Add numerical values: Interpolate, extrapolate, average, median, or much better TAKE DISTRIBUTION INTO ACCOUNT.   \n",
    "What will happen if too many missing values are replaced by the median?  \n",
    "Add categorical values, by existing features (E.g.: find gender by name, email, BMI and age combination, combine all of these, then  maybe use descriptive stats).\n",
    "But, if there are a lot of missing values are in the features that matter most => better drop the observations totally. E.g. gender and preferences, for customer segmentation.     \n",
    "There is also [imputation as a function of the other features](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation).\n",
    "\n",
    "## Data augmentation (add rows of observations).\n",
    "There are libraries for it. Random data generation is also a thing.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec35b5-98ec-4a29-b7b6-e8e5820c591f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Data Preproccesing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "\n",
    "### Scaling   \n",
    "[Some definitions, and should I scale?](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
    "Standardization, Normalize (Unit norm scaling). [Scale to defined range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)\n",
    "\n",
    "### Binning, [Discretization](https://scikit-learn.org/stable/modules/preprocessing.html#discretization)   \n",
    "Discretization: separete continous variable to discrete. Then can be encoded to (ordered) categorical.\n",
    "\n",
    "### Encoding (Numerical to Categorical and vice versa, Ordinal, Label Encoding, One Hot Encoding)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badcdb5-df42-4bbc-bc41-ac1b8ba9bec7",
   "metadata": {},
   "source": [
    "**Train, validate, debug, repeat, repeat, repeat.***   \n",
    "\n",
    "Finally, **test once** in totally \"unknown\" dataset. Reality check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee861adb-14a5-4f85-9999-36672d567a5c",
   "metadata": {},
   "source": [
    "## [Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html), dimensionality reduction, [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca).\n",
    "Supervised VS [unsupervied dimensionality reduction](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).    \n",
    "\n",
    "Degree of Sparsity of Coefficients: number of \"non-zero\" coefficients in a model relative to the total coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173043ae-c004-4ade-9bd8-08854b9f9c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
