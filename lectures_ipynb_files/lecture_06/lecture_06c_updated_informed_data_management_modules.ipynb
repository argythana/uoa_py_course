{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b62dda-e473-40e2-bc93-d57925ea7b7d",
   "metadata": {},
   "source": [
    "# Lecture 6c: Updated, informed data management modules. Advanced material.   \n",
    "\n",
    "**Optional for the purpose of this course, necessary if you aim to work with python and Data Science.**\n",
    "\n",
    "**Work in Progress**, *thanks to class of 2024 for always asking for more.*  \n",
    "\n",
    "Generated by asking the right prompts to an AI, curated, corrected and filtered by you know who.     \n",
    "The best example is always at the end.  \n",
    "\n",
    "Note: Upload the full version on eclass, after 50% of students submitted their practise sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917327f-61fa-41ec-abf5-301427f88cd6",
   "metadata": {},
   "source": [
    "## 1. More data management modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d4173-791b-44eb-9958-b8a765bf6314",
   "metadata": {},
   "source": [
    "### 1.2 Multithreading and RAM efficiency: Polars   \n",
    "More efficient as it does not load all the data into RAM at once.   \n",
    "Pandas and Polars are both data analysis libraries in Python, but they have some key differences:\n",
    "\n",
    "1. **Performance**: Polars is designed to be more efficient than Pandas, especially when dealing with large datasets. It uses a lazy evaluation approach, which means that computations are not performed until the result is needed. This can lead to significant performance improvements.\n",
    "\n",
    "2. **Memory Usage**: Polars is also more memory efficient than Pandas. It does not load all the data into RAM at once, but instead loads only the chunks of data that are needed for computation. This can be a big advantage when working with large datasets that do not fit into memory.\n",
    "\n",
    "3. **API Compatibility**: While Polars provides a similar API to Pandas for data manipulation, there are some differences. If you're used to working with Pandas, you might need to learn new ways of doing things in Polars.\n",
    "\n",
    "4. **Multi-threading**: Polars is designed to utilize multi-threading, which means it can use all available CPU cores for computations. This can lead to faster execution times compared to Pandas, which is single-threaded.\n",
    "\n",
    "In summary, if you're working with larger datasets and find that Pandas is too slow or uses too much memory, Polars might be a good alternative to consider. However, for smaller datasets, the performance difference might not be noticeable, and using Pandas could be simpler and more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31f91f-8b0f-447e-bafb-28f224b20aea",
   "metadata": {},
   "source": [
    "### 1.3 Big data frameworks: PySpark\n",
    "\n",
    "Pandas and PySpark are both data analysis libraries in Python, but they have some key differences:\n",
    "\n",
    "1. **Data Size**: Pandas is typically used for small to medium-sized datasets that can fit into memory. On the other hand, PySpark is designed for distributed computing on big data using the Apache Spark framework. It can handle much larger datasets that don't fit into memory.\n",
    "\n",
    "2. **Performance**: PySpark can be faster than Pandas when dealing with large datasets because it uses the Spark engine for distributed computing. It can distribute the data and computations across multiple nodes in a cluster, which can significantly speed up processing times.\n",
    "\n",
    "3. **API**: While both provide similar data manipulation capabilities, their APIs are different. If you're used to working with Pandas, you might need to learn new ways of doing things in PySpark.\n",
    "\n",
    "4. **Environment**: PySpark is typically used in a distributed computing environment (like Hadoop or Spark), while Pandas is used in a single-machine environment. \n",
    "\n",
    "5. **Functionality**: PySpark provides more functionality beyond data manipulation, including support for distributed machine learning and graph processing through libraries like MLlib and GraphX.\n",
    "\n",
    "In summary, if you're working with larger datasets and need distributed computing capabilities, PySpark might be a good choice. However, for smaller datasets and simpler data analysis tasks, Pandas could be simpler and more straightforward to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f2e8a-5a03-40b3-a3ce-74d7f628dede",
   "metadata": {},
   "source": [
    "### 1.4 Other alternatives to PySpark for big data.\n",
    "**CuPy**, **Vaex**, and **Datatable** are all libraries in Python that are used for data manipulation and analysis, similar to Pandas. However, they each have their unique features and use-cases:\n",
    "\n",
    "1. **CuPy**: CuPy is a GPU-accelerated library for numerical computations. It provides a numpy-like interface while offloading heavy computations to GPUs for better performance. It's particularly useful for tasks that require heavy numerical computations. However, it doesn't provide the same data manipulation capabilities as Pandas.\n",
    "\n",
    "2. **Vaex**: Vaex is a high performance Python library for lazy, out-of-core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can be used to visualize and manipulate large datasets (beyond the capacity of Pandas) efficiently as it uses memory mapping, a lazy approach, and a functional programming style to achieve optimal performance.\n",
    "\n",
    "3. **Datatable**: Datatable is a library in Python that is designed for big data processing. It's similar to Pandas but is designed to be more efficient with larger datasets. It uses a columnar storage approach (data is stored by columns rather than by rows), which can make certain operations much faster. It's also multithreaded, which can provide a significant speed boost over Pandas.\n",
    "\n",
    "In summary, while all these libraries can be used for data manipulation and analysis, the choice between them depends on your specific use case.    \n",
    "* If you're dealing with very large datasets, Vaex or Datatable might be more suitable.    \n",
    "* If your work involves heavy numerical computations, CuPy might be the better choice.\n",
    "* For general data manipulation tasks, especially with smaller to medium-sized datasets, Pandas is often the go-to library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09667f7-2c43-4054-9407-1648b3b22121",
   "metadata": {},
   "source": [
    "### 1.5 Other alternatives to PySpark for big data and distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef5e7-76bb-430f-a8bf-cd4b2547086f",
   "metadata": {},
   "source": [
    "Yes, there are several alternatives to PySpark that are used for distributed data processing. Here are a few:\n",
    "\n",
    "1. **Dask**: Dask is a flexible library for parallel computing in Python. It's built with the Python ecosystem in mind and integrates well with tools like NumPy, Pandas, and Scikit-Learn. Dask provides dynamic task scheduling and parallel collections that extend the functionality of these tools to larger datasets.\n",
    "\n",
    "2. **Ray**: Ray is a general-purpose distributed computing framework. In addition to simple map-reduce style computations, Ray supports task dependencies and also provides libraries for distributed training of machine learning models, hyperparameter tuning, reinforcement learning and more.\n",
    "\n",
    "3. **Koalas**: Koalas is a project that brings the pandas API to Apache Spark. It aims to make the transition from pandas to Spark easier by providing a familiar API.\n",
    "\n",
    "4. **Vaex**: Vaex is a Python library for lazy, out-of-core DataFrames, similar to pandas, but designed to be used with big data. It uses memory mapping, a lazy approach, and a functional programming style to achieve optimal performance.\n",
    "\n",
    "5. **Modin**: Modin is a library that speeds up your pandas workflows by changing a single line of code. It uses parallelism to accomplish this, via Ray or Dask.\n",
    "\n",
    "Remember to choose the tool that best fits your specific needs and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e7dfc-9c7f-44c3-8618-c707692b59c5",
   "metadata": {},
   "source": [
    "### 1.6 Scalable pandas code: Modin   \n",
    "Modin and Pandas are both data analysis libraries in Python, but they have some key differences:\n",
    "\n",
    "1. **Performance**: Modin is designed to speed up your Pandas workflows by changing a single line of code. It uses parallelism to accomplish this, via Ray or Dask. This means that operations in Modin are designed to utilize all available CPU cores, which can lead to significant speed improvements for larger datasets.\n",
    "\n",
    "2. **API Compatibility**: Modin is a drop-in replacement for Pandas, meaning it aims to have complete API compatibility with Pandas. This means you can use Modin as a direct replacement for Pandas in your code, and it should work as expected. However, there might be some edge cases where Modin's behavior is slightly different from Pandas.\n",
    "\n",
    "3. **Scalability**: While Pandas is excellent for small to medium-sized datasets, it can struggle with larger datasets. Modin, on the other hand, is designed to handle larger datasets more efficiently by distributing the computations across all cores.\n",
    "\n",
    "4. **Memory Usage**: Modin can be more memory efficient than Pandas, especially for larger datasets. This is because Modin partitions the data and only loads the partitions that are necessary for computation, reducing memory usage.\n",
    "\n",
    "In summary, if you're working with larger datasets and find that Pandas is too slow or uses too much memory, Modin might be a good alternative to consider. However, for smaller datasets, the performance difference might not be noticeable, and using Pandas could be simpler and more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0121d1-f2ec-4a91-88ae-647113e550e6",
   "metadata": {},
   "source": [
    "## 2. Comparing modules for various use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87602a5-373e-46a8-9010-0394c5b53c3f",
   "metadata": {},
   "source": [
    "### 2.1 Comparing Modin versus Polars.\n",
    "\n",
    "1. **Performance and Scalability**: Both Modin and Polars are designed to handle larger datasets more efficiently than traditional Pandas. Modin achieves this by parallelizing computations across all available CPU cores using either Ray or Dask. Polars, on the other hand, uses a lazy evaluation approach, which means that computations are not performed until the result is needed. This can lead to significant performance improvements, especially when dealing with large datasets.\n",
    "\n",
    "2. **Memory Usage**: Polars is more memory efficient than Modin. It does not load all the data into RAM at once, but instead loads only the chunks of data that are needed for computation. This can be a big advantage when working with large datasets that do not fit into memory. Modin, while more memory efficient than Pandas, still loads the entire dataset into memory.\n",
    "\n",
    "3. **API Compatibility**: Modin aims to be a drop-in replacement for Pandas, meaning it strives for complete API compatibility with Pandas. This makes transitioning from Pandas to Modin relatively straightforward. Polars, while providing a similar API to Pandas, does have some differences. If you're used to working with Pandas, you might need to learn new ways of doing things in Polars.\n",
    "\n",
    "4. **Multi-threading**: Both Modin and Polars are designed to utilize multi-threading, which means they can use all available CPU cores for computations. This can lead to faster execution times compared to single-threaded libraries like Pandas.\n",
    "\n",
    "In summary, if you're working with larger datasets and find that Pandas is too slow or uses too much memory, both Modin and Polars might be good alternatives to consider. The choice between Modin and Polars will depend on your specific needs and constraints, such as memory usage, API familiarity, and the specific nature of your data and computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a34e9-c54a-4903-914e-73f7bba99875",
   "metadata": {},
   "source": [
    "#### 2.1.1. Advantages of Polars over Modin.\n",
    "\n",
    "Polars and Modin are both data analysis libraries in Python designed to handle larger datasets more efficiently than traditional Pandas. However, they have some key differences and advantages over each other:\n",
    "\n",
    "1. **Memory Efficiency**: Polars is more memory efficient than Modin. Polars does not load all the data into RAM at once, but instead loads only the chunks of data that are needed for computation. This can be a big advantage when working with large datasets that do not fit into memory. Modin, while more memory efficient than Pandas, still loads the entire dataset into memory.\n",
    "\n",
    "2. **Lazy Evaluation**: Polars uses a lazy evaluation approach, which means that computations are not performed until the result is needed. This can lead to significant performance improvements, especially when dealing with large datasets. Modin, on the other hand, performs computations immediately.\n",
    "\n",
    "3. **Multi-threading**: Both Polars and Modin are designed to utilize multi-threading, which means they can use all available CPU cores for computations. However, Polars is designed from the ground up to be multi-threaded, which can lead to more efficient utilization of CPU cores.\n",
    "\n",
    "4. **API Differences**: While Modin aims to be a drop-in replacement for Pandas, meaning it strives for complete API compatibility with Pandas, Polars provides a similar but not identical API to Pandas. This means that transitioning from Pandas to Polars might require learning new ways of doing things, but it also means that Polars can offer features and functionality that are not available in Pandas or Modin.\n",
    "\n",
    "In summary, the advantages of Polars over Modin mainly lie in its memory efficiency, lazy evaluation approach, and potentially more efficient multi-threading. However, the choice between Polars and Modin will depend on your specific needs and constraints, such as memory usage, API familiarity, and the specific nature of your data and computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b367622-9436-48a8-8a71-428cb3df0ea9",
   "metadata": {},
   "source": [
    "#### 2.1.2. Advantages of Modin over Polars.   \n",
    "\n",
    "Modin and Polars are both data analysis libraries in Python designed to handle larger datasets more efficiently than traditional Pandas. However, they have some key differences and advantages over each other:\n",
    "\n",
    "1. **API Compatibility**: Modin aims to be a drop-in replacement for Pandas, meaning it strives for complete API compatibility with Pandas. This makes transitioning from Pandas to Modin relatively straightforward. Polars, while providing a similar API to Pandas, does have some differences. If you're used to working with Pandas, you might need to learn new ways of doing things in Polars.\n",
    "\n",
    "2. **Parallelism Frameworks**: Modin provides the flexibility to choose the parallelism framework that suits your needs. It supports both Ray and Dask as backends, which allows you to choose based on your specific use case or environment. Polars, on the other hand, does not offer this flexibility.\n",
    "\n",
    "3. **Ease of Use**: Given its goal to be a drop-in replacement for Pandas, Modin can be easier to use for those who are already familiar with the Pandas API. The transition from Pandas to Modin can be as simple as changing a single line of code in some cases.\n",
    "\n",
    "4. **Community and Ecosystem**: Modin is part of the Ray Project ecosystem, which is a fast-growing project with a large community and a wide range of tools for distributed computing. This can be an advantage in terms of finding help, resources, and integrations with other tools.\n",
    "\n",
    "In summary, the advantages of Modin over Polars mainly lie in its API compatibility with Pandas, flexibility in choosing parallelism frameworks, ease of use, and its community and ecosystem. However, the choice between Modin and Polars will depend on your specific needs and constraints, such as API familiarity, the specific nature of your data and computations, and your computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777841ab-7c3f-421c-80a8-8df1d2289736",
   "metadata": {},
   "source": [
    "## 3. The PyArrow datatypes on pandas\n",
    "[Apache Arrow development platform for in-memory analytics](https://arrow.apache.org/docs/python/index.html)   \n",
    "\n",
    "[PyArrow functionality](https://pandas.pydata.org/docs/user_guide/pyarrow.html#pyarrow-functionality)   \n",
    "\n",
    "**To use this functionality, please ensure you have installed the [minimum supported PyArrow version.](https://pandas.pydata.org/docs/getting_started/install.html#install-optional-dependencies)**   \n",
    "It is one of pandas optional dependecies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603874a2-7b55-4a60-b028-4b0ef4a92dd0",
   "metadata": {},
   "source": [
    "## 4. Always stay up-to-date on python data management. Examples for data modules that generalize for all python ecosystem.   \n",
    "!TODO: Split this notebook, refactor as lecture 6 Instructions. (Think about it?).     \n",
    "Or, just add corresponding examples in each lecture (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d156d8-bbda-4b82-baba-93fc5b3a842e",
   "metadata": {},
   "source": [
    "### 4.1. Read the official docs.   \n",
    "e.g. https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e6d6f-9e96-42c8-bf4a-d399bfda8c71",
   "metadata": {},
   "source": [
    "### 4.2 Signup and watch relevant conferences (PyData).  \n",
    "https://pydata.org/global2023/   \n",
    "\n",
    "https://www.youtube.com/@PyDataTV  \n",
    "[PyData Global 2023 complete video gallery](https://www.youtube.com/playlist?list=PLGVZCDnMOq0rCyO6B53u1eFT4owN8Lvwj)\n",
    "\n",
    "[PyData Global 2022 complete video gallery](https://www.youtube.com/playlist?list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960b03a-e009-42a9-822e-6b203141b35d",
   "metadata": {},
   "source": [
    "### 4.3 Ask questions in stackoverflow.   \n",
    "[How to ask a good question](https://stackoverflow.com/help/how-to-ask)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c6025-1159-4a66-a24f-a75846539952",
   "metadata": {},
   "source": [
    "### 4.4. Follow the right people and join the right communities in social media.   \n",
    "Cassie Kozyrkov.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090dea0-b756-4442-b4b4-b147995eead0",
   "metadata": {},
   "source": [
    "### 4.5. Double check AI copilots and assistants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112b528-e8e2-4f00-9d84-b14842467b60",
   "metadata": {},
   "source": [
    "`argythana:`  \n",
    "> append_df = df1.append(df2)  AttributeError: 'DataFrame' object has no attribute 'append'\n",
    "> Why is this wrong?   \n",
    "\n",
    "\n",
    "`GitHub Copilot:`\n",
    "> The error message you're seeing typically indicates that the method you're trying to use doesn't exist for the object you're using it on. However, the `append` method is a valid method for a DataFrame object in pandas.\n",
    "> The error might be due to a different reason such as:\n",
    "> 1. You might have overwritten the pandas DataFrame class with a variable of the same name. Make sure you haven't used `DataFrame` as a variable elsewhere in your code.\n",
    "> 2. There might be an issue with your pandas installation. Try reinstalling pandas using pip or conda.\n",
    "> 3. You might be using an older version of pandas where the `append` method is not available. You can check your pandas version with `print(pd.__version__)`. If it's not the latest version, consider upgrading it.\n",
    "> If none of these are the case, please provide more context or check the rest of your code for potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b2973-4a57-4f5a-90b1-1d109618effa",
   "metadata": {},
   "source": [
    "`argythana:`   \n",
    "\n",
    "> when was df.append() method deprecated and when whas it totally removed from pandas?\n",
    "\n",
    "`GitHub Copilot:`\n",
    "> The `append()` method is not deprecated and has not been removed from pandas. It is still a valid method for appending rows of one DataFrame to the end of another DataFrame, returning a new object. The error you're encountering might be due to a different reason. Please ensure that `df1` and `df2` are indeed pandas DataFrames. If the issue persists, consider checking your pandas installation or version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218d30-4afb-411c-9866-141994823672",
   "metadata": {},
   "source": [
    "`argythana on 2nd of April, 2024:`  \n",
    "> which version of github copilot is this?\n",
    "\n",
    "`GitHub Copilot:`  \n",
    "> This is the GPT-4 version of GitHub Copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3f4c3-0c2f-414d-a09e-4cd93721a704",
   "metadata": {},
   "source": [
    "Meanwhile, since pandas 1.4 (January 22, 2022) `df.append()` has been [deprecated](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.DataFrame.append.html).        \n",
    "\n",
    "\n",
    "`df.append()` has been removed for ages, since April 3, 2023.   \n",
    "Read this [reply.](https://stackoverflow.com/a/75956237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee465b3-62df-4547-b0c4-c551d384a365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
