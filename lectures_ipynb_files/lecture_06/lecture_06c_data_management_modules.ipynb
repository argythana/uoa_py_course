{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe50edf1-2be9-4985-bd96-5aa133952a4e",
   "metadata": {},
   "source": [
    "# Auto pandas-profiling\n",
    "\n",
    "https://pypi.org/project/pandas-profiling/   \n",
    "\n",
    "https://pypi.org/project/ydata-profiling/\n",
    "\n",
    "Currently https://github.com/ydataai/ydata-profiling\n",
    "\n",
    "https://docs.profiling.ydata.ai/latest/\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/generate-reports-using-pandas-profiling-deploy-using-streamlit/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5fb86-da75-439d-826d-567bc7d6f00e",
   "metadata": {},
   "source": [
    "# tools that generate reports similar to pandas-profiling:\n",
    "\n",
    "1. **Sweetviz**: Sweetviz is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with a single line of code. Output is a fully self-contained HTML application.\n",
    "\n",
    "2. **Dtale**: D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals.\n",
    "\n",
    "3. **Autoviz**: Autoviz is an open-source Python library that is particularly effective for rapid data visualization. With just one line of code, Autoviz can visualize any dataset, no matter the size.\n",
    "\n",
    "4. **Facets**: Facets contains two robust visualizations to aid in understanding and analyzing machine learning datasets. Facets Overview gives a high-level view of one or more datasets, and Facets Dive allows users to interactively explore up to tens of thousands of datapoints.\n",
    "\n",
    "Remember to choose the tool that best fits your specific needs and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d4173-791b-44eb-9958-b8a765bf6314",
   "metadata": {},
   "source": [
    "# Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e7dfc-9c7f-44c3-8618-c707692b59c5",
   "metadata": {},
   "source": [
    "# Modin   \n",
    "Modin and Pandas are both data analysis libraries in Python, but they have some key differences:\n",
    "\n",
    "1. **Performance**: Modin is designed to speed up your Pandas workflows by changing a single line of code. It uses parallelism to accomplish this, via Ray or Dask. This means that operations in Modin are designed to utilize all available CPU cores, which can lead to significant speed improvements for larger datasets.\n",
    "\n",
    "2. **API Compatibility**: Modin is a drop-in replacement for Pandas, meaning it aims to have complete API compatibility with Pandas. This means you can use Modin as a direct replacement for Pandas in your code, and it should work as expected. However, there might be some edge cases where Modin's behavior is slightly different from Pandas.\n",
    "\n",
    "3. **Scalability**: While Pandas is excellent for small to medium-sized datasets, it can struggle with larger datasets. Modin, on the other hand, is designed to handle larger datasets more efficiently by distributing the computations across all cores.\n",
    "\n",
    "4. **Memory Usage**: Modin can be more memory efficient than Pandas, especially for larger datasets. This is because Modin partitions the data and only loads the partitions that are necessary for computation, reducing memory usage.\n",
    "\n",
    "In summary, if you're working with larger datasets and find that Pandas is too slow or uses too much memory, Modin might be a good alternative to consider. However, for smaller datasets, the performance difference might not be noticeable, and using Pandas could be simpler and more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31f91f-8b0f-447e-bafb-28f224b20aea",
   "metadata": {},
   "source": [
    "## big data \n",
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f2e8a-5a03-40b3-a3ce-74d7f628dede",
   "metadata": {},
   "source": [
    "## Other alternatives\n",
    "**CuPy**, **Vaex**, and **Datatable** are all libraries in Python that are used for data manipulation and analysis, similar to Pandas. However, they each have their unique features and use-cases:\n",
    "\n",
    "1. **CuPy**: CuPy is a GPU-accelerated library for numerical computations. It provides a numpy-like interface while offloading heavy computations to GPUs for better performance. It's particularly useful for tasks that require heavy numerical computations. However, it doesn't provide the same data manipulation capabilities as Pandas.\n",
    "\n",
    "2. **Vaex**: Vaex is a high performance Python library for lazy, out-of-core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can be used to visualize and manipulate large datasets (beyond the capacity of Pandas) efficiently as it uses memory mapping, a lazy approach, and a functional programming style to achieve optimal performance.\n",
    "\n",
    "3. **Datatable**: Datatable is a library in Python that is designed for big data processing. It's similar to Pandas but is designed to be more efficient with larger datasets. It uses a columnar storage approach (data is stored by columns rather than by rows), which can make certain operations much faster. It's also multithreaded, which can provide a significant speed boost over Pandas.\n",
    "\n",
    "In summary, while all these libraries can be used for data manipulation and analysis, the choice between them depends on your specific use case. If you're dealing with very large datasets, Vaex or Datatable might be more suitable. If your work involves heavy numerical computations, CuPy might be the better choice. For general data manipulation tasks, especially with smaller to medium-sized datasets, Pandas is often the go-to library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef5e7-76bb-430f-a8bf-cd4b2547086f",
   "metadata": {},
   "source": [
    "Yes, there are several alternatives to PySpark that are used for distributed data processing. Here are a few:\n",
    "\n",
    "1. **Dask**: Dask is a flexible library for parallel computing in Python. It's built with the Python ecosystem in mind and integrates well with tools like NumPy, Pandas, and Scikit-Learn. Dask provides dynamic task scheduling and parallel collections that extend the functionality of these tools to larger datasets.\n",
    "\n",
    "2. **Ray**: Ray is a general-purpose distributed computing framework. In addition to simple map-reduce style computations, Ray supports task dependencies and also provides libraries for distributed training of machine learning models, hyperparameter tuning, reinforcement learning and more.\n",
    "\n",
    "3. **Koalas**: Koalas is a project that brings the pandas API to Apache Spark. It aims to make the transition from pandas to Spark easier by providing a familiar API.\n",
    "\n",
    "4. **Vaex**: Vaex is a Python library for lazy, out-of-core DataFrames, similar to pandas, but designed to be used with big data. It uses memory mapping, a lazy approach, and a functional programming style to achieve optimal performance.\n",
    "\n",
    "5. **Modin**: Modin is a library that speeds up your pandas workflows by changing a single line of code. It uses parallelism to accomplish this, via Ray or Dask.\n",
    "\n",
    "Remember to choose the tool that best fits your specific needs and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f331ff-2727-4d6e-a416-11cf31c8636a",
   "metadata": {},
   "source": [
    "# tools to interact with Databases\n",
    "\n",
    "python for sql\n",
    "\n",
    "https://duckdb.org/2023/02/24/jupysql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37b00b-24aa-4e74-8dcf-933d15f98085",
   "metadata": {},
   "source": [
    "https://github.com/ploomber/jupysql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
